
## üöÄ **Model Deployment and Serving ‚Äì Theory**

### **1Ô∏è‚É£ Model Serving**

Model Serving refers to **making a trained ML model available to real users or applications** for predictions.

Examples:

* Recommending products on an e-commerce site
* Fraud detection during credit-card transaction
* Real-time translation of text

Serving mechanisms:

| Method                       | Description                                                  |
| ---------------------------- | ------------------------------------------------------------ |
| Batch Inference              | Run predictions periodically on large dataset, store results |
| Real-time (Online) Inference | Serve predictions instantly via API                          |
| Stream Inference             | Handle live data streams in milliseconds                     |

A model server usually exposes:

```
POST /predict
{
  "input": {...}
}
```

---

### **2Ô∏è‚É£ Model Drift & Online / Offline Serving**

#### üîπ What is Model Drift?

Over time, a model becomes **less accurate** because:

* Data distribution changes
* User behavior changes
* Market/seasonal trends change

Types of drift:

| Type          | Example                                       |
| ------------- | --------------------------------------------- |
| Data Drift    | New features, changed value distribution      |
| Concept Drift | Relationship between input and output changes |

To reduce drift:

* Monitor model performance
* Retrain model periodically or automatically

#### üîπ Online vs Offline Serving

| Type                | Suitable for                      | Architecture                          |
| ------------------- | --------------------------------- | ------------------------------------- |
| **Offline Serving** | Batch predictions, scheduled jobs | Data ‚Üí Model ‚Üí Storage ‚Üí App          |
| **Online Serving**  | Instant real-time predictions     | App ‚Üí Model API ‚Üí Prediction response |

---

### **3Ô∏è‚É£ Model Deployment and Serving**

Deployment is the **process of moving a model from development to production**.

Typical MLOps deployment workflow:

```
MLflow Registry (model version) ‚Üí CI/CD Pipeline ‚Üí Model Server (container) ‚Üí Production
```

Deployment Platforms:

* Kubernetes (most common)
* AWS Sagemaker / Azure ML / GCP Vertex AI
* BentoML (open-source)
* TorchServe / TensorFlow Serving / ONNX Runtime

Key production considerations:

| Requirement   | Why                       |
| ------------- | ------------------------- |
| Low latency   | Real-time predictions     |
| Auto-scaling  | Traffic spikes            |
| Security      | Protect model APIs & data |
| Observability | Track performance & drift |

---

### **4Ô∏è‚É£ Demo Concept: Model Serving using BentoML**

**BentoML** is an open-source framework to **package, deploy and serve ML models**.

What BentoML does:
‚úî Converts model into a production-ready API service
‚úî Supports all major ML frameworks (sklearn, PyTorch, TensorFlow, XGBoost etc.)
‚úî Generates Docker images automatically

Typical flow:

```
Train Model ‚Üí Save model using MLflow ‚Üí Import into BentoML ‚Üí Create service ‚Üí Deploy
```

BentoML creates artifacts:

* `bentofile.yaml` (deployment definition)
* Auto-generated Docker image
* REST gRPC inference server

---

### **5Ô∏è‚É£ Demo Concept: Upgrading Model Versions with BentoML Serving**

When a new model version is trained:

```
Model v1 (Production) ‚Üí serving traffic
Model v2 (Staging) ‚Üí evaluated in shadow mode / canary deployment
```

Upgrade techniques:

| Strategy          | Description                                        |
| ----------------- | -------------------------------------------------- |
| Rolling update    | Replace old version gradually                      |
| Blue-Green        | Run v1 and v2 separately ‚Üí switch after confidence |
| Shadow Testing    | v2 runs in parallel without affecting users        |
| Canary Deployment | Send small portion of traffic to v2, then increase |

If v2 underperforms ‚Üí rollback to **v1**.

---

### **6Ô∏è‚É£ Monitoring Tools (Prometheus, Grafana, Evidently)**

Monitoring in ML is not only about **infrastructure** ‚Äî we also track **model correctness**.

| Tool             | Focus Area                                  |
| ---------------- | ------------------------------------------- |
| **Prometheus**   | Collects metrics (latency, CPU, throughput) |
| **Grafana**      | Visual dashboards for monitoring metrics    |
| **Evidently AI** | Detects data drift & performance drift      |

Important KPIs in production:

| Category | Metrics                                  |
| -------- | ---------------------------------------- |
| Infra    | CPU, RAM, request latency, error rate    |
| Business | Click-through rate, conversion rate      |
| ML model | Accuracy, precision, recall, drift score |

‚ö†Ô∏è When performance drops ‚Üí trigger **retraining pipeline**.

---

## üî• Summary

| Stage         | Purpose                                  |
| ------------- | ---------------------------------------- |
| Model Serving | Expose model for inference               |
| Model Drift   | Detect accuracy degradation over time    |
| Deployment    | Move model from dev to prod reliably     |
| BentoML       | Build, package & deploy ML inference API |
| Model Upgrade | Rollout new versions safely              |
| Monitoring    | Track infra + application + ML drifts    |

---

